{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Adapt our files to be the right size for the model\n",
    "def preprocess_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=1)\n",
    "    img = tf.image.resize(img, (528, 528))  # Resize the image\n",
    "    img = tf.cast(img, tf.float32) / 127.5 - 1.0  # Normalize to [-1, 1] range, similar to RGB preprocessing\n",
    "    img = tf.image.grayscale_to_rgb(img)  # Convert grayscale to RGB by duplicating the channel\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_datasets(image_folder, label_file, batch_size=32):\n",
    "    pitch_image_paths = []\n",
    "    pitch_labels = []\n",
    "    duration_image_paths = []\n",
    "    duration_labels = []\n",
    "\n",
    "    with open(label_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        previous_filename = None\n",
    "        for row in reader:\n",
    "            filename = row[0]\n",
    "            labels = [int(x) for x in row[1][1:-1].split(', ')]\n",
    "\n",
    "            # If the filename matches the previous one, it's a duration label\n",
    "            if filename == previous_filename:\n",
    "                duration_image_paths.append(os.path.join(image_folder, filename))\n",
    "                duration_labels.append(labels)\n",
    "            else:\n",
    "                pitch_image_paths.append(os.path.join(image_folder, filename))\n",
    "                pitch_labels.append(labels)\n",
    "\n",
    "            previous_filename = filename\n",
    "\n",
    "    # Creating the datasets\n",
    "    pitch_dataset = tf.data.Dataset.from_tensor_slices((pitch_image_paths, pitch_labels))\n",
    "    duration_dataset = tf.data.Dataset.from_tensor_slices((duration_image_paths, duration_labels))\n",
    "\n",
    "    # Applying preprocessing to images\n",
    "    pitch_dataset = pitch_dataset.map(lambda x, y: (preprocess_image(x), y))\n",
    "    duration_dataset = duration_dataset.map(lambda x, y: (preprocess_image(x), y))\n",
    "\n",
    "    # Batching and prefetching\n",
    "    pitch_dataset = pitch_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    duration_dataset = duration_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return pitch_dataset, duration_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB6\n",
    "from tensorflow.keras.layers import Reshape, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def get_cnn_model():\n",
    "    base_model = EfficientNetB6(\n",
    "        input_shape=(528, 528, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\"\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze the model to use it as a feature extractor\n",
    "\n",
    "    # Reshape the output to prepare it for the Transformer Encoder\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "\n",
    "    cnn_model = Model(base_model.input, base_model_out)\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = get_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, MultiHeadAttention, Dense, LayerNormalization, Embedding, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "class TransformerEncoderBlock(Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = tf.keras.Sequential([\n",
    "            Dense(dense_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "\n",
    "        ])\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_output = self.attention(inputs, inputs)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "class TransformerDecoderBlock(Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, vocab_size):\n",
    "        super().__init__()\n",
    "        self.attention_1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "        self.layernorm_3 = LayerNormalization()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.dropout_1 = Dropout(0.3)\n",
    "        self.dropout_2 = Dropout(0.5)\n",
    "        self.out = Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs)\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(query=out_1, value=encoder_outputs, key=encoder_outputs)\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "        ffn_output = self.ffn(out_2)\n",
    "        ffn_output = self.dropout_1(ffn_output, training=training)\n",
    "        return self.out(ffn_output)\n",
    "\n",
    "class MusicGenerationModel(tf.keras.Model):\n",
    "    def __init__(self, cnn_model):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = TransformerEncoderBlock(embed_dim=128, dense_dim=512, num_heads=8)\n",
    "        self.decoder = TransformerDecoderBlock(embed_dim=128, ff_dim=512, num_heads=8, vocab_size=22)\n",
    "\n",
    "    def call(self, image, target):\n",
    "        cnn_features = self.cnn_model(image)\n",
    "        encoded_features = self.encoder(cnn_features)\n",
    "        output = self.decoder(target, encoded_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicGenerationModel(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "pitch_dataset, duration_dataset = generate_datasets('../raw_data/sheet_images', '../raw_data/labels.csv')\n",
    "\n",
    "# Define the model architecture (same as before)\n",
    "pitch_model = MusicGenerationModel(cnn_model)\n",
    "duration_model = MusicGenerationModel(cnn_model)\n",
    "\n",
    "# Compile the models\n",
    "pitch_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "duration_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the models separately\n",
    "pitch_model.fit(pitch_dataset, epochs=25, validation_data=None)  # Add validation data if available\n",
    "duration_model.fit(duration_dataset, epochs=25, validation_data=None)  # Add validation data if available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features from CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_feature_extractor = tf.keras.Model(inputs=pitch_model.input, outputs=pitch_model.cnn_model.output)\n",
    "duration_feature_extractor = tf.keras.Model(inputs=duration_model.input, outputs=duration_model.cnn_model.output)\n",
    "\n",
    "# Concatenate the outputs along the last dimension\n",
    "def combine_features(pitch_features, duration_features):\n",
    "    combined_features = tf.concat([pitch_features, duration_features], axis=-1)\n",
    "    return combined_features\n",
    "\n",
    "# Assuming `transformer_encoder` is your Transformer encoder model\n",
    "combined_features = combine_features(pitch_feature_extractor(image_input), duration_feature_extractor(image_input))\n",
    "encoded_output = transformer_encoder(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to integrate the entire model\n",
    "\n",
    "class CompleteMusicGenerationModel(tf.keras.Model):\n",
    "    def __init__(self, pitch_feature_extractor, duration_feature_extractor, transformer_encoder, transformer_decoder):\n",
    "        super().__init__()\n",
    "        self.pitch_feature_extractor = pitch_feature_extractor\n",
    "        self.duration_feature_extractor = duration_feature_extractor\n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        self.transformer_decoder = transformer_decoder\n",
    "\n",
    "    def call(self, image, target_sequence):\n",
    "        pitch_features = self.pitch_feature_extractor(image)\n",
    "        duration_features = self.duration_feature_extractor(image)\n",
    "        combined_features = combine_features(pitch_features, duration_features)\n",
    "        encoded_output = self.transformer_encoder(combined_features)\n",
    "        decoded_output = self.transformer_decoder(target_sequence, encoded_output)\n",
    "        return decoded_output\n",
    "\n",
    "# Initialize and compile the complete model\n",
    "complete_model = CompleteMusicGenerationModel(\n",
    "    pitch_feature_extractor=pitch_feature_extractor,\n",
    "    duration_feature_extractor=duration_feature_extractor,\n",
    "    transformer_encoder=transformer_encoder,\n",
    "    transformer_decoder=transformer_decoder\n",
    ")\n",
    "\n",
    "complete_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `train_dataset` has both image inputs and target sequences\n",
    "complete_model.fit(train_dataset, epochs=25, validation_data=validation_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to MusicXML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream, note\n",
    "\n",
    "def predict_sequence(image):\n",
    "    sequence = model.predict(image)\n",
    "    return decode_sequence(sequence)  # Implement a function to decode sequences\n",
    "\n",
    "def convert_to_musicxml(sequence):\n",
    "    s = stream.Stream()\n",
    "    for sym in sequence:\n",
    "        if sym == 'C4 Quarter':\n",
    "            n = note.Note('C4', quarterLength=1.0)\n",
    "            s.append(n)\n",
    "        # Handle other symbols similarly\n",
    "    s.write('musicxml', fp='output.xml')\n",
    "\n",
    "# Inference\n",
    "new_image = load_image('path_to_image')  # Load a new image\n",
    "predicted_sequence = predict_sequence(new_image)\n",
    "convert_to_musicxml(predicted_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
