{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=1)\n",
    "    img = tf.image.resize(img, (528, 528))  # Resize the image\n",
    "    img = tf.cast(img, tf.float32) / 127.5 - 1.0  # Normalize to [-1, 1] range\n",
    "    img = tf.image.grayscale_to_rgb(img)  # Convert grayscale to RGB by duplicating the channel\n",
    "    return img\n",
    "\n",
    "# Define the function to encode pitch and duration into a single integer\n",
    "def encode_labels(pitch, duration, num_durations):\n",
    "    return pitch * num_durations + duration\n",
    "\n",
    "# Generate datasets\n",
    "def generate_datasets(image_folder, label_file, batch_size=32, num_durations=5):\n",
    "    image_paths = []\n",
    "    encoded_labels = []\n",
    "\n",
    "    with open(label_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip the header\n",
    "        for row in reader:\n",
    "            filename = row[0]\n",
    "            labels = ast.literal_eval(row[1])  # Parse the string to get the list of tuples\n",
    "\n",
    "            # Encode each (pitch, duration) pair into a single integer\n",
    "            labels_encoded = [encode_labels(p, d, num_durations) for p, d in labels]\n",
    "\n",
    "            # Add to lists\n",
    "            image_paths.append(os.path.join(image_folder, filename))\n",
    "            encoded_labels.append(labels_encoded)\n",
    "\n",
    "    # Create TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, encoded_labels))\n",
    "\n",
    "    # Apply preprocessing to images and pack image and label together\n",
    "    dataset = dataset.map(lambda x, y: ((preprocess_image(x), y), y))  # Pack image and label for model input\n",
    "\n",
    "    # Batching and prefetching\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "pitch_dataset = generate_datasets('../raw_data/sheet_images', '../raw_data/labels.csv', batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB6\n",
    "from tensorflow.keras.layers import Reshape, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# CNN model (feature extractor)\n",
    "def get_cnn_model():\n",
    "    base_model = EfficientNetB6(\n",
    "        input_shape=(528, 528, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\"\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze the model to use it as a feature extractor\n",
    "\n",
    "    # Reshape the output to prepare it for the Transformer Encoder\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "\n",
    "    cnn_model = Model(base_model.input, base_model_out)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, MultiHeadAttention, Dense, LayerNormalization, Embedding, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "class TransformerEncoderBlock(Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = tf.keras.Sequential([\n",
    "            Dense(dense_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_output = self.attention(inputs, inputs)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "# Transformer Decoder Block\n",
    "class TransformerDecoderBlock(Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, vocab_size):\n",
    "        super().__init__()\n",
    "        self.attention_1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "        self.layernorm_3 = LayerNormalization()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.dropout_1 = Dropout(0.3)\n",
    "        self.dropout_2 = Dropout(0.5)\n",
    "        self.out = Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs)\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(query=out_1, value=encoder_outputs, key=encoder_outputs)\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "        ffn_output = self.ffn(out_2)\n",
    "        ffn_output = self.dropout_1(ffn_output, training=training)\n",
    "        return self.out(ffn_output)\n",
    "\n",
    "# Music Generation Model\n",
    "class MusicGenerationModel(tf.keras.Model):\n",
    "    def __init__(self, cnn_model):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = TransformerEncoderBlock(embed_dim=128, dense_dim=512, num_heads=8)\n",
    "        self.decoder = TransformerDecoderBlock(embed_dim=128, ff_dim=512, num_heads=8, vocab_size=85)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        image, target = inputs  # Unpack the inputs\n",
    "        cnn_features = self.cnn_model(image)\n",
    "        encoded_features = self.encoder(cnn_features)\n",
    "        output = self.decoder(target, encoded_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = get_cnn_model()\n",
    "music_model = MusicGenerationModel(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pitch_dataset, duration_dataset \u001b[38;5;241m=\u001b[39m generate_datasets(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../raw_data/sheet_images\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../raw_data/labels.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the model architecture (same as before)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pitch_model \u001b[38;5;241m=\u001b[39m MusicGenerationModel(cnn_model)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "pitch_dataset, duration_dataset = generate_datasets('../raw_data/sheet_images', '../raw_data/labels.csv')\n",
    "\n",
    "# Define the model architecture (same as before)\n",
    "pitch_model = MusicGenerationModel(cnn_model)\n",
    "duration_model = MusicGenerationModel(cnn_model)\n",
    "\n",
    "# Compile the models\n",
    "pitch_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "duration_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the models separately\n",
    "pitch_model.fit(pitch_dataset, epochs=25, validation_data=None)  # Add validation data if available\n",
    "duration_model.fit(duration_dataset, epochs=25, validation_data=None)  # Add validation data if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninjamac/.pyenv/versions/3.10.6/envs/Consonance/lib/python3.10/site-packages/keras/src/layers/layer.py:1331: UserWarning: Layer 'music_generation_model_9' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling TransformerEncoderBlock.call().\n",
      "\n",
      "\u001b[1mDimensions must be equal, but are 2304 and 128 for '{{node transformer_encoder_block_9_1/add_1}} = AddV2[T=DT_FLOAT](transformer_encoder_block_9_1/layer_normalization_48_1/add_2, transformer_encoder_block_9_1/sequential_19_1/dense_48_1/Add)' with input shapes: [?,289,2304], [?,289,128].\u001b[0m\n",
      "\n",
      "Arguments received by TransformerEncoderBlock.call():\n",
      "  • inputs=tf.Tensor(shape=(None, 289, 2304), dtype=float32)''\n",
      "  warnings.warn(\n",
      "/Users/ninjamac/.pyenv/versions/3.10.6/envs/Consonance/lib/python3.10/site-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'music_generation_model_9', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TransformerEncoderBlock.call().\n\n\u001b[1mDimensions must be equal, but are 2304 and 128 for '{{node music_generation_model_9_1/transformer_encoder_block_9_1/add_1}} = AddV2[T=DT_FLOAT](music_generation_model_9_1/transformer_encoder_block_9_1/layer_normalization_48_1/add_2, music_generation_model_9_1/transformer_encoder_block_9_1/sequential_19_1/dense_48_1/Add)' with input shapes: [?,289,2304], [?,289,128].\u001b[0m\n\nArguments received by TransformerEncoderBlock.call():\n  • inputs=tf.Tensor(shape=(None, 289, 2304), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m music_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmusic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpitch_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add validation data if available\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Consonance/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[36], line 60\u001b[0m, in \u001b[0;36mMusicGenerationModel.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m image, target \u001b[38;5;241m=\u001b[39m inputs  \u001b[38;5;66;03m# Unpack the inputs\u001b[39;00m\n\u001b[1;32m     59\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_model(image)\n\u001b[0;32m---> 60\u001b[0m encoded_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(target, encoded_features)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[36], line 19\u001b[0m, in \u001b[0;36mTransformerEncoderBlock.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m proj_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_1(inputs \u001b[38;5;241m+\u001b[39m attention_output)\n\u001b[1;32m     18\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_proj(proj_input)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_2(\u001b[43mproj_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproj_output\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling TransformerEncoderBlock.call().\n\n\u001b[1mDimensions must be equal, but are 2304 and 128 for '{{node music_generation_model_9_1/transformer_encoder_block_9_1/add_1}} = AddV2[T=DT_FLOAT](music_generation_model_9_1/transformer_encoder_block_9_1/layer_normalization_48_1/add_2, music_generation_model_9_1/transformer_encoder_block_9_1/sequential_19_1/dense_48_1/Add)' with input shapes: [?,289,2304], [?,289,128].\u001b[0m\n\nArguments received by TransformerEncoderBlock.call():\n  • inputs=tf.Tensor(shape=(None, 289, 2304), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "music_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "music_model.fit(pitch_dataset, epochs=25, validation_data=None)  # Add validation data if available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features from CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_feature_extractor = tf.keras.Model(inputs=pitch_model.input, outputs=pitch_model.cnn_model.output)\n",
    "duration_feature_extractor = tf.keras.Model(inputs=duration_model.input, outputs=duration_model.cnn_model.output)\n",
    "\n",
    "# Concatenate the outputs along the last dimension\n",
    "def combine_features(pitch_features, duration_features):\n",
    "    combined_features = tf.concat([pitch_features, duration_features], axis=-1)\n",
    "    return combined_features\n",
    "\n",
    "# Assuming `transformer_encoder` is your Transformer encoder model\n",
    "combined_features = combine_features(pitch_feature_extractor(image_input), duration_feature_extractor(image_input))\n",
    "encoded_output = transformer_encoder(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to integrate the entire model\n",
    "\n",
    "class CompleteMusicGenerationModel(tf.keras.Model):\n",
    "    def __init__(self, pitch_feature_extractor, duration_feature_extractor, transformer_encoder, transformer_decoder):\n",
    "        super().__init__()\n",
    "        self.pitch_feature_extractor = pitch_feature_extractor\n",
    "        self.duration_feature_extractor = duration_feature_extractor\n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        self.transformer_decoder = transformer_decoder\n",
    "\n",
    "    def call(self, image, target_sequence):\n",
    "        pitch_features = self.pitch_feature_extractor(image)\n",
    "        duration_features = self.duration_feature_extractor(image)\n",
    "        combined_features = combine_features(pitch_features, duration_features)\n",
    "        encoded_output = self.transformer_encoder(combined_features)\n",
    "        decoded_output = self.transformer_decoder(target_sequence, encoded_output)\n",
    "        return decoded_output\n",
    "\n",
    "# Initialize and compile the complete model\n",
    "complete_model = CompleteMusicGenerationModel(\n",
    "    pitch_feature_extractor=pitch_feature_extractor,\n",
    "    duration_feature_extractor=duration_feature_extractor,\n",
    "    transformer_encoder=transformer_encoder,\n",
    "    transformer_decoder=transformer_decoder\n",
    ")\n",
    "\n",
    "complete_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `train_dataset` has both image inputs and target sequences\n",
    "complete_model.fit(train_dataset, epochs=25, validation_data=validation_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to MusicXML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream, note\n",
    "\n",
    "def predict_sequence(image):\n",
    "    sequence = model.predict(image)\n",
    "    return decode_sequence(sequence)  # Implement a function to decode sequences\n",
    "\n",
    "def convert_to_musicxml(sequence):\n",
    "    s = stream.Stream()\n",
    "    for sym in sequence:\n",
    "        if sym == 'C4 Quarter':\n",
    "            n = note.Note('C4', quarterLength=1.0)\n",
    "            s.append(n)\n",
    "        # Handle other symbols similarly\n",
    "    s.write('musicxml', fp='output.xml')\n",
    "\n",
    "# Inference\n",
    "new_image = load_image('path_to_image')  # Load a new image\n",
    "predicted_sequence = predict_sequence(new_image)\n",
    "convert_to_musicxml(predicted_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
